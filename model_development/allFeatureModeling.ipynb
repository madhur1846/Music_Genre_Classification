{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 27 02:45:16 2020\n",
    "\n",
    "@author: joygu\n",
    "\"\"\"\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# change the file locatio here\n",
    "train_path = \"data/pickle/train/\"\n",
    "test_path = \"data/pickle/test/\"\n",
    "#load train data\n",
    "# need reshap\n",
    "chroma = pickle.load( open( train_path + \"chroma_stft.p\", \"rb\" ) )\n",
    "mfcc = pickle.load( open( train_path + \"mfcc.p\", \"rb\" ) )\n",
    "#not need\n",
    "spectral_centroid = pickle.load( open( train_path + \"spectral_centroid.p\", \"rb\" ) )\n",
    "spectral_rolloff = pickle.load( open( train_path + \"spectral_rolloff.p\", \"rb\" ) )\n",
    "zcr = pickle.load( open( train_path + \"zcr.p\", \"rb\" ) )\n",
    "\n",
    "#load test data\n",
    "# need reshap\n",
    "chroma_test = pickle.load( open( test_path + \"chroma_stft.p\", \"rb\" ) )\n",
    "mfcc_test = pickle.load( open( test_path + \"mfcc.p\", \"rb\" ) )\n",
    "#not need\n",
    "spectral_centroid_test = pickle.load( open( test_path + \"spectral_centroid.p\", \"rb\" ) )\n",
    "spectral_rolloff_test = pickle.load( open( test_path + \"spectral_rolloff.p\", \"rb\" ) )\n",
    "zcr_test = pickle.load( open( test_path + \"zcr.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# find data point used in this small data set\n",
    "list_key=list(zcr.keys())\n",
    "list_key_test=list(zcr_test.keys())\n",
    "\n",
    "#%% define function\n",
    "def preprocess_data(list_key, feature):\n",
    "    cleaned_feature_dict = defaultdict(np.array)\n",
    "    cleaned_feature=[]\n",
    "    for path in list_key:\n",
    "        feature_result = dict(feature)[path]\n",
    "        _, n_frames = feature_result.shape\n",
    "        if n_frames < 2500: ## Remove all shorter mp3 files from analysis\n",
    "            continue\n",
    "        feature_result_fixed = feature_result[:, :2582] ## Resizes all np.arrays to 2582 frames\n",
    "        scale = StandardScaler()\n",
    "        scaled_feature_result = scale.fit_transform(feature_result_fixed)\n",
    "        feature_result_long = scaled_feature_result.reshape(1,-1)\n",
    "        cleaned_feature_dict[path] = feature_result_long\n",
    "        cleaned_feature.append(feature_result_long)\n",
    "    new_list_key = dict(cleaned_feature_dict).keys()\n",
    "# return new data path and dictionary of its corresponding value\n",
    "    return list(new_list_key), cleaned_feature_dict,cleaned_feature\n",
    "\n",
    "def nonmatrix_data(list_key, feature):\n",
    "    cleaned_feature=[]\n",
    "    for path in list_key:\n",
    "        feature_result = dict(feature)[path]\n",
    "        feature_result_fixed = np.array(feature_result[:, :2582]).reshape(-1,1) ## Resizes all np.arrays to 2582 frames\n",
    "        scale = StandardScaler()\n",
    "        scaled_feature_result = scale.fit_transform(feature_result_fixed)\n",
    "        cleaned_feature.append(scaled_feature_result.reshape(1,-1))\n",
    "# return new data path and dictionary of its corresponding value\n",
    "    return cleaned_feature\n",
    "\n",
    "def combinefeature(datalen,arr1,arr2,arr3,arr4,arr5):\n",
    "    overallfeature=[]\n",
    "    for x in range(datalen):\n",
    "        a,b,c,d,e = arr1[x],arr2[x],arr3[x],arr4[x],arr5[x]\n",
    "        overallfeature.append(np.concatenate((a,b,c,d,e), axis=1))\n",
    "    return overallfeature\n",
    "\n",
    "def combinefeature1(datalen,arr1,arr2,arr3,arr4,arr5):\n",
    "    overallfeature=[]\n",
    "    for x in range(datalen):\n",
    "        a,b,c,d,e = arr1[x],arr2[x],arr3[x],arr4[x],arr5[x]\n",
    "        a= np.array(arr1[x]).reshape(-1,1)[:10000].reshape(1,-1)\n",
    "        b=np.array(arr2[x]).reshape(-1,1)[:10000].reshape(1,-1)\n",
    "        c=np.array(arr3[x]).reshape(-1,1)[:2582].reshape(1,-1)\n",
    "        d=np.array(arr4[x]).reshape(-1,1)[:2582].reshape(1,-1)\n",
    "        e=np.array(arr5[x]).reshape(-1,1)[:2582].reshape(1,-1)\n",
    "        overallfeature.append(np.concatenate((a,b,e), axis=1))\n",
    "    return overallfeature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% data preprocessing\n",
    "list_key,_, chromafeature = preprocess_data(list_key, chroma)\n",
    "list_key,_, mfccfeature = preprocess_data(list_key, mfcc)\n",
    "sc_feature = nonmatrix_data(list_key, spectral_centroid)\n",
    "sr_feature = nonmatrix_data(list_key, spectral_rolloff)\n",
    "zcr_feature = nonmatrix_data(list_key, zcr)\n",
    "ttl_feature  = combinefeature(len(list_key),chromafeature,mfccfeature,sc_feature,sr_feature,zcr_feature)\n",
    "#ttl_feature  = combinefeature1(len(list_key),chromafeature,mfccfeature,zcr_feature)\n",
    "\n",
    "list_key_test,_, chromafeature_test = preprocess_data(list_key_test, chroma_test)\n",
    "list_key_test,_, mfccfeature_test = preprocess_data(list_key_test, mfcc_test)\n",
    "sc_feature_test = nonmatrix_data(list_key_test, spectral_centroid_test)\n",
    "sr_feature_test = nonmatrix_data(list_key_test, spectral_rolloff_test)\n",
    "zcr_feature_test = nonmatrix_data(list_key_test, zcr_test)\n",
    "ttl_feature_test  = combinefeature(len(list_key_test),chromafeature_test,mfccfeature_test,sc_feature_test,sr_feature_test,zcr_feature_test)\n",
    "#ttl_feature_test  = combinefeature1(len(list_key_test),chromafeature_test,mfccfeature_test,zcr_feature_test)\n",
    "\n",
    "train_genere_id = [float(list_key[z].split('/')[-1][:-4]) for z in range(len(list_key))]\n",
    "test_genere_id = [float(list_key_test[z].split('/')[-1][:-4]) for z in range(len(list_key_test))]\n",
    "total_genere_id =[]\n",
    "total_genere_id.extend(train_genere_id)\n",
    "total_genere_id.extend(test_genere_id)\n",
    "#train_feature = list(feature.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% data preprocessing 2\n",
    "TRACKS_PATH = '/Users/joygu/Documents/summer2020/isye6740/project/music-genre-classification-master/data/fma_metadata/tracks.csv'\n",
    "tracks = []\n",
    "trackid_t=[]\n",
    "heads2=[]\n",
    "i=1\n",
    "rawdata = []\n",
    "with open(TRACKS_PATH) as csvfile:\n",
    "    readCSV1 = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV1:\n",
    "        if i<4:\n",
    "            readhead=row\n",
    "            heads2.append(readhead)\n",
    "        else:    \n",
    "            readentry = [x for x in row]#[float(x) for x in row]\n",
    "            rawdata.append(readentry)\n",
    "            trackid_t.append(float(readentry[0]))\n",
    "            tracks.append(readentry[40])\n",
    "        i=i+1\n",
    "index=np.searchsorted(trackid_t,total_genere_id)\n",
    "tracks_new=[tracks[index[small]] for small in range(len(index))]\n",
    " # label each catagory with numbers\n",
    "le = LabelEncoder()\n",
    "genre_id = le.fit_transform(tracks_new)\n",
    "classes=list(le.classes_)\n",
    "# fit the classes to this small data set\n",
    "y_train, y_test = genre_id[:len(train_genere_id)],genre_id[len(train_genere_id):-1]\n",
    "y_test=np.append(y_test,genre_id[-1])\n",
    "\n",
    "\n",
    "x_train = np.array(ttl_feature).reshape(6384,-1)\n",
    "x_test = np.array(ttl_feature_test).reshape(1596,-1)\n",
    "print('training start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "for Model in [GaussianNB, KNeighborsClassifier,MLPClassifier,LogisticRegression,SVC]:\n",
    "#for Model in [MLPClassifier]:\n",
    "    print(Model)\n",
    "    #gaussian 0.34~ knn 0.45~ logistic default converge error? SVC 0.4775~ NN 0.455\n",
    "    if Model == GaussianNB:\n",
    "        clf =  GaussianNB(var_smoothing=1.29e-10)\n",
    "    elif Model == LogisticRegression:\n",
    "        #clf =  LogisticRegression(solver='liblinear')\n",
    "        clf =  LogisticRegression(solver='liblinear',max_iter=1000)#max_iter=5000)\n",
    "    elif Model == KNeighborsClassifier:\n",
    "        clf = Model(n_neighbors=21)\n",
    "    elif Model == MLPClassifier:\n",
    "        clf =  Model( activation='relu', max_iter=1000, hidden_layer_sizes=(140,2), random_state=1)\n",
    "    elif Model == SVC:\n",
    "        clf = Model(kernel = 'rbf',max_iter=2000)\n",
    "        #clf = Model(kernel='linear', C=1.0,max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = np.array(clf.predict(x_test),dtype=np.int)\n",
    "    y_expected = np.array(y_test,dtype=np.int)\n",
    "    print(np.sum(y_pred==y_expected)/y_expected.shape[0])\n",
    "    err.append(np.sum(y_pred==y_expected)/y_expected.shape[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
